[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nbdev_example",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "nbdev_example",
    "section": "Install",
    "text": "Install\npip install nbdev_example"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "nbdev_example",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2\n\n\nTest comment"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Collective-risk Dilemmas",
    "section": "",
    "text": "Note: I don’t use classes much. Instead, I use python dictionaries as data structures and use a functional programming style similar to common LISP or Clojure.\nA Collective-risk dilemma (CRD) is a game which captures a situation where a disaster may occur should players fail to collectively attain sufficiently high contributions from the players. Contributions are costly, so there is an incentive to refuse to contribute and free-ride on the contributions of others.\nThe Collective-risk dilemma posed by (Domingos et al (2019))[10.1016/j.simpat.2021.102299] introduces additional considerations: - 1. Timing uncertainty: the dilemma may have a non-zero chance in each round of remaining unresolved. The same dilemma plays out for another round if so (the authors presuppose that players do not discount the value of future rounds). - 2. Impact uncertainty: there is a non-zero chance of a disaster if contributions are too low. Also, the damage caused by the disaster could have a relatively large margin for error. - 3. Threshold uncertainty: there is uncertainty over how high total contributiosn need to be to avoid disaster.\nsource"
  },
  {
    "objectID": "core.html#solution-methods-from-game-theory",
    "href": "core.html#solution-methods-from-game-theory",
    "title": "Collective-risk Dilemmas",
    "section": "Solution Methods from Game Theory",
    "text": "Solution Methods from Game Theory\nHow will players act in a Collective-risk Dilemma?\nThere are many formal methods for specifying how players choose to behave in a game. - Classical Game Theory largely focuses on the non-cooperative solution concept of a Nash Equilibrium, where no one can do better by deviating unilaterally from their current behavior. - Evolutionary Game Theory considers the distribution of actions players take in the limit of rare mutations. - Cooperative Game Theory focuses on finding fair ways to allocate the benefits the interaction. - Reinforcement Learning and methods such as Experience-weighted attraction consider how players act as they learn about the game.\nHere, we will focus on Reinforcement Learning.\nYou can find a treatment of other methods in the relevant textbooks: TBC"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods in Evolutionary Game Theory",
    "section": "",
    "text": "A set of functions that I commonly use in notebooks\n\n\ndef thread_macro(current_value, *funcs, identifier=\"self\"):\n    \"\"\"Pipes current_value through each function in funcs.\n\n    Each element in funcs is either a function or a list/tuple containing\n    a function followed by its other arguments.\n    This function imitates the Clojure as-if threading macro.\n\n    Notes: By default current_value is threaded as the first argument of the\n    function call. Yet, one can use the syntax [func, arg1, \"self\", arg2] (or\n    (func, arg1, \"self\", arg2)) so that current_value will instead be threaded\n    in whatever place \"self\" would be. If you need to, you can set this \"self\"\n    identifier to a different value.\n    \"\"\"\n\n    for func in funcs:\n        if isinstance(func, (list, tuple)):\n            place = 0\n            for i, el in enumerate(func[1:]):\n                if el == identifier:\n                    place = i\n                    func = [el for el in func if el != identifier]\n            func, args1, args2 = func[0], func[1:place + 1], func[place + 1:]\n            current_value = func(*args1, current_value, *args2)\n        else:\n            current_value = func(current_value)\n    return current_value\n\n\ndef broadcast_concatenate_axes(ax1, ax2):\n    \"\"\"Broadcast both numpy axes and concatenate along last dimension\"\"\"\n    ax1new = ax1\n    for _ in range(np.ndim(ax2) - 1):\n        ax1new = ax1new[..., None, :]\n    ax2new = ax2\n    for _ in range(np.ndim(ax1) - 1):\n        ax2new = ax2new[None, ..., :]\n    ax1new = np.broadcast_to(ax1new,\n                             (*ax1.shape[:-1], *ax2.shape[:-1], ax1.shape[-1]))\n    ax2new = np.broadcast_to(ax2new,\n                             (*ax1.shape[:-1], *ax2.shape[:-1], ax2.shape[-1]))\n    ax = np.concatenate((ax1new, ax2new), axis=-1)\n    return ax\n\n\ndef build_parameter_grid_from_axes(axes):\n    \"\"\"Build a numpy array with all combinations of elements specified in axes.\n\n    Each axis in axes gives an array of values that should be repeated for each\n    value in the other axes. Primitive types and lists of primitive types are\n    first promoted to numpy arrays.\n    \"\"\"\n\n    dtypes = (float, int, bool, str)\n    for i, axis in enumerate(axes):\n        condition = isinstance(axis, dtypes) or all(\n            isinstance(el, dtypes) for el in list(axis))\n        axes[i] = np.array([axis]).T if condition else axis\n    tensor = functools.reduce(broadcast_concatenate_axes, axes)\n    return tensor.reshape((-1, tensor.shape[-1]))"
  },
  {
    "objectID": "methods.html#finite-markov-chains",
    "href": "methods.html#finite-markov-chains",
    "title": "Methods in Evolutionary Game Theory",
    "section": "Finite Markov Chains",
    "text": "Finite Markov Chains\nWe examine a finite population of players using different strategies who engage in social learning.\nIn the limit of small mutations, most of the time everyone plays the same strategy. States in which everyone plays the same strategy are known as monomorphic states. Occassionally, mutant strategies can fixate in the population, resulting in everyone adopting the same new strategy. We can use Markov Chains to analyse the relative frequencies with which each strategy is played by the population.\nThe steps for computing the ergodic (i.e. long-run) strategy distribution is as follows:\n\nBuild a transition matrix between all monomorphic states\nFind the unit eigenvector associated with this transition matrix\n\n\nFermi social learning\n\nA Fermi social learning rule means that individuals make pairwise comparisons between their own strategy and and another strategy in the population that they may choose to copy.\n\n\nDerivation\nEach period of the evolutionary game involves individuals being randomly selected to play against one another individual.\nLetting \\(Z\\) denote the size of the population, and \\(π\\) denote the game’s payoff matrix, we can compute the fitness of a strategy, \\(B\\) for example, when \\(k\\) individuals are of type \\(B\\) as follows:\n\\[\\begin{equation}\nΠB_k = πBA \\frac{k-1}{Z} + πBB \\frac{Z-k}{Z}\n\\end{equation}\\]\nwhere \\(πBA\\) and \\(πBB\\) are the payoffs for playing \\(B\\) against type \\(A\\) or \\(B\\) respectively.\nThe Fermi social learning rule adopts strategy \\(B\\) selected from the population over their current strategy \\(A\\) with probability given by:\n\\[\\begin{equation}\nPr(adopt \\, B | k) = \\frac{1}{(1 + \\exp^{-\\beta (ΠB_k - ΠA_k)})}\n\\end{equation}\\]\nwhere \\(ΠB_k - ΠA_k\\) is the relative fitness of strategy \\(B\\) over \\(A\\) in a population with \\(k\\) individuals of type \\(B\\), the rest of type \\(A\\). Notice how the larger the relative fitness, the closer the denominator, and therefore the probability, is to \\(1\\).\nUsing the Fermi social learning rule above, we can write the probability of increasing the number of type \\(A\\) individuals as\n\\[\\begin{equation}\nT^+_B = \\frac{Z-k}{Z} \\frac{k}{Z} Pr(adopt \\, B | k)\n\\end{equation}\\]\nas an individual of type \\(A\\) needs to randomly be chosen to compare their strategy against someone of type \\(B\\).\nand the probability of decreasing the number of type \\(B\\) individuals as\n\\[\\begin{equation}\nT^-_B = \\frac{k}{Z} \\frac{Z-k}{Z} Pr(adopt \\, A | k)\n\\end{equation}\\]\nas an individual of type \\(B\\) needs to randomly be chosen to compare their strategy against someone of type \\(A\\).\n\n\nDefinition\n\ndef fermi_learning(β:nptyping.NDArray, # learning rate\n                   fitnessA:nptyping.NDArray, # fitness of strategy A\n                   fitnessB:nptyping.NDArray # fitness of strategy B\n                  ) -> nptyping.NDArray:\n    \"\"\"Compute the likelihood that a player with strategy B adopts strategy A using the fermi function.\"\"\"\n    return (1 + np.exp(-β*(fitnessA - fitnessB)))**-1\n\n\n# show_doc(fermi_learning)\n\n\n\nExamples and Tests\nWhen each strategy has the same fitness, then the likelihoodthat a player adopts strategy \\(A\\) is 50%, no matter the value of \\(\\beta\\).\n\nx = fermi_learning(np.array([1]), \n                   np.array([5]),\n                   np.array([5]))\nnptyping.assert_isinstance(x, nptyping.NDArray[nptyping.Shape[\"1\"], typing.Any])\ntest_eq(x, 0.5)\n\n\n\n\nFixation rate\n\nThe fixation rate for type B in a population of type A is defined as the probability that the appearance of a mutant of type B leads to the entire population adopting type B instead of A, i.e. what is the likelihood that a mutant of type A invades population B.\n\n\nDerivation\nA derivation of the fixation rate defined below can be found in Nowak 2006 (reproduced below).\n\nConsider a one-dimensional stochastic process on a discrete state space, $ i {0, 1, , N}$ that represents the number of individuals in a population of \\(N\\) individuals who are of type \\(A\\), the rest are type \\(B\\).\nIn each stochastic event, the number of individuals of type \\(A\\) can at most increase or decrease by 1.\nFor a given number of individuals, \\(i\\), let \\(a_i\\), \\(b_i\\), and \\(1 - a_i - b_i\\) represent the chance of an increase, decrease, or no change in \\(i\\).\nThis stochastic process follows the transition matrix ,\\(P\\) (not to be confused with the transition matrices we discuss elsewhere!)\n\\[\\begin{equation}\nP \\, = \\, \\begin{pmatrix}\n1 & 0 & 0 & \\cdots & 0 & 0 & 0\\\\\nb_1 & (1 - a_1 - b_1) & a_1 & \\cdots & 0 & 0 & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & b_{n-1} & (1 - a_{n-1} - b_{n-1}) & a_{n-1}\\\\\n0 & 0 & 0 & \\cdots & 0 & 0 & 1\\\\\n\\end{pmatrix}\n\\end{equation}\\]\nDenote by \\(x_i\\) the probability of reaching state \\(N\\) when starting from \\(i\\).\nFrom transition matrix \\(P\\) above, we can see that \\(x_i\\) must satisfy:\n\\(x_0 = 0\\)\n\\(x_i = b_i x_{i-1} + (1 - a_i - b_i) x_i + a_i x_{i+1}\\)\n\\(x_N = 1\\)\nThe fixation rate for a mutant A in a population of type B is clearly \\(x_1\\)\nWe can solve for \\(x_i\\) by rewriting the above as \\(b_i x_i - b_i x_{i-1} = a_i x_{i+1} - a_i x_i\\).\nWe can denote \\(y_i = x_i - x_{i-1}\\) to simplify the above to \\(y_{i+1} = \\frac{b_i}{a_i} y_i\\)\nNotice that \\(\\sum_{i=1}^N{y_i} = x_N - x_0 = 1\\) and that \\(y_1 = x_1\\)\nWe can use the above to write \\[\\begin{equation}\nx_1 + {\\sum_{i=2}^N{y_i}} = x_1 (1 + {\\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}}}) = 1\n\\end{equation}\\]\nAnd so \\[\\begin{equation}\nx_1 = \\frac{1}{(1 + \\sum_{i=1}^{N-1}{\\prod_{j=1}^{i} \\frac{b_j}{a_j}})}\n\\end{equation}\\]\nNote that \\(x_1\\) is the fixation rate for a mutant \\(A\\) in a population of type \\(B\\)\nAlso note that \\(x_{N-1}\\) is the fixation rate for a mutant \\(B\\) in a population of type \\(A\\). We could find expressions for all \\(x_i\\) if we note that \\(x_i = x_1 (1 + \\sum_{j=1}^{i-1}{\\prod_{k=1}^{j} \\frac{b_k}{a_k}})\\) (see Nowak 2006 for further details).\n\n\n\nDefinition\n\nT_type = list[nptyping.NDArray[nptyping.Shape[\"N_models, N_strategies, N_strategies\"], typing.Any]]\n\ndef fixation_rate(Tplus: T_type, # A list of NDarrays, one array for each possible number of mutants in the population, each array should be of shape (n_models, n_strategies, n_strategies) for computing all fixation rates for each model; the probability of gaining one mutant\n                  Tneg: T_type, # A list of NDarrays, one array for each possible number of mutants in the population, each array should be of shape (n_models, n_strategies, n_strategies) for computing all fixation rates for each model; the probability of losing one mutant\n                 ) -> nptyping.NDArray[nptyping.Shape[\"N_models, N_strategies, N_strategies\"], typing.Any]:\n    \"\"\"Calculate the likelihood that a mutant invades the population.\"\"\"\n    Z = len(Tplus) - 1\n    ρ = (np.sum([np.prod([Tneg[j]/Tplus[j]\n                         for j in range(1,i+1)],\n                        axis=0,\n                        keepdims=False)\n                 for i in range(1,Z)],\n                axis=0,\n                keepdims=False)\n        + 1)**-1\n    return ρ\n\n\n# show_doc(fixation_rate)\n\n\n\nExamples and Tests\nWhen the chance of gaining a mutant equals the chance of losing a mutant, then the fixation rate will be 50%\n\nTplus_example = [np.array([0.4]), \n                 np.array([0.4]),\n                 np.array([0.4])]\nTneg_example =  [np.array([0.4]), \n                 np.array([0.4]),\n                 np.array([0.4])]\n\n\nfixation_rate_result = fixation_rate(Tplus_example, Tneg_example)\nnptyping.assert_isinstance(fixation_rate_result,\n                           nptyping.NDArray[nptyping.Shape[\"1\"], typing.Any])\ntest_eq(fixation_rate_result,\n        np.array([0.5]))\n\nWhen the chance of gaining a mutant is half the chance of losing a mutant, then the fixation rate will be 1/3\n\nTplus_example = [np.array([0.2]), \n                 np.array([0.2]),\n                 np.array([0.2])]\nTneg_example =  [np.array([0.4]), \n                 np.array([0.4]),\n                 np.array([0.4])]\n\n\nfixation_rate_result = fixation_rate(Tplus_example, Tneg_example)\nnptyping.assert_isinstance(fixation_rate_result,\n                           nptyping.NDArray[nptyping.Shape[\"1\"], typing.Any])\ntest_eq(fixation_rate_result,\n        np.array([1/3]))\n\n\n\n\nBuild transition matrix\nhttps://stackoverflow.com/questions/59498869/how-can-i-type-hint-a-nested-object-in-python\n\ndef build_transition_matrix(models):\n    \"\"\"Build a transition matrix between all monomorphic states\n    using the fermi social learning rule for each model.\"\"\"\n    \n    Z, S, β = [models[k] for k in ['Z','strategy_set', 'β']]\n    π = models['payoffs']\n    n_models = π.shape[0]\n    M = np.zeros(( n_models, len(S), len(S)))\n    for row_ind, s in enumerate(S):\n        for col_ind, sₒ in enumerate(S):\n            if row_ind == col_ind:\n                # We compute these entries later\n                continue\n            πAA = π[:, row_ind, row_ind]\n            πAB = π[:, row_ind, col_ind]\n            πBA = π[:, col_ind, row_ind]\n            πBB = π[:, col_ind, col_ind]\n            ΠA = [πAA*(k-1)/Z + πAB*(Z-k)/Z\n                  for k in range(Z+1)]\n            ΠB = [πBA*k/Z + πBB*(Z-k-1)/Z\n                  for k in range(Z+1)]\n            Tplus = [(Z - k)/Z\n                     * k/Z\n                     * fermi_learning(β, ΠA[k], ΠB[k])\n                     for k in range(Z+1)]\n            Tneg = [(Z - k)/Z\n                    * k/Z\n                    * fermi_learning(β, ΠB[k], ΠA[k])\n                    for k in range(Z+1)]\n            ρ = fixation_rate(Tplus, Tneg)\n            M[:, col_ind, row_ind] = ρ / max(1, len(S)-1)\n    for row_ind in range(len(S)):\n        col_inds = [i for i in range(len(S)) if i != row_ind]\n        no_move = 1 - np.sum(M[:, row_ind, col_inds], axis=1)\n        M[:, row_ind, row_ind] = no_move\n    return {**models, \"transition_matrix\": M}\n\n\n\nFind ergodic strategy distribution\n\ndef find_ergodic_distribution(models):\n    \"\"\"Find the ergodic distribution of a markov chain with the\n    given transition matrix.\"\"\"\n    \n    M = models.get(\"transition_matrix\", np.zeros((1, 1, 1)))\n    # find unit eigenvector of markov chain\n    Λ,V = np.linalg.eig(M.transpose(0,2,1))\n    x = np.isclose(Λ, 1)\n    # if multiple unit eigenvalues then choose the first\n    y = np.zeros_like(x, dtype=bool)\n    idx = np.arange(len(x)), x.argmax(axis=1)\n    y[idx] = x[idx]\n    ergodic = np.array(V.transpose(0,2,1)[y], dtype=float)\n    # ensure ergodic frequencies are positive and sum to 1\n    ergodic = np.abs(ergodic) / np.sum(np.abs(ergodic), axis=1)[:, None]\n    return {**models, 'ergodic':ergodic}\n\n\n\nRun full markov chain algorithm\n\ndef markov_chain(models):\n    \"\"\"Find the ergodic distribution of the evolutionary\n    game given by each model in models.\"\"\"\n    return thread_macro(models,\n                        build_transition_matrix,\n                        find_ergodic_distribution)"
  },
  {
    "objectID": "methods.html#todo",
    "href": "methods.html#todo",
    "title": "Methods in Evolutionary Game Theory",
    "section": "TODO",
    "text": "TODO\n\nadd unit tests for markov_chain\nadd integration tests for makov_chain\nMove utilities to its own notebook or to its own package"
  }
]